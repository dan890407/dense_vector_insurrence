{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CMKrR2hUu2An",
        "outputId": "e9b9d4b6-45ba-4d6e-a220-d922d5259d00"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Downloading: 100%|██████████| 110k/110k [00:00<00:00, 1.07MB/s]\n",
            "c:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:123: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Dan\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Downloading: 100%|██████████| 29.0/29.0 [00:00<00:00, 29.0kB/s]\n",
            "Downloading: 100%|██████████| 624/624 [00:00<00:00, 625kB/s]\n",
            "Downloading: 100%|██████████| 412M/412M [04:30<00:00, 1.52MB/s] \n",
            "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "c:\\Users\\Dan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\tokenization_utils_base.py:2302: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[3 2 1 0]\n",
            "Document:  荷蘭隊會在今年經典賽大敗給中華隊\n",
            "Cosine similarity score:  0.9976505\n",
            "\n",
            "Document:  統一獅會是今年中華職棒總冠軍，我超級喜歡看球賽\n",
            "Cosine similarity score:  0.9968725\n",
            "\n",
            "Document:  免疫學拓荒者韓韶華追思會 醫界齊聚緬懷共同導師\n",
            "Cosine similarity score:  0.9958317\n",
            "\n",
            "Document:  稱俄烏戰爭「是針對我們發動的」 俄外長發言引哄堂大笑\n",
            "Cosine similarity score:  0.9941991\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertModel\n",
        "import numpy as np\n",
        "import json\n",
        "# Load the pre-trained tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')\n",
        "#\n",
        "model = BertModel.from_pretrained('bert-base-chinese')\n",
        "# Define the corpus of documents\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## define corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus = [\"稱俄烏戰爭「是針對我們發動的」 俄外長發言引哄堂大笑\", \"免疫學拓荒者韓韶華追思會 醫界齊聚緬懷共同導師\", \"統一獅會是今年中華職棒總冠軍，我超級喜歡看球賽\", \"荷蘭隊會在今年經典賽大敗給中華隊\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Tokenize and encode the documents\n",
        "max_length = 512\n",
        "token_ids_list = [tokenizer.encode(doc, add_special_tokens=True, max_length=max_length, pad_to_max_length=True) for doc in corpus]\n",
        "tokens_tensor = torch.tensor(token_ids_list)\n",
        "\n",
        "# Get the BERT model output for the token IDs\n",
        "with torch.no_grad():\n",
        "    output = model(tokens_tensor)\n",
        "\n",
        "# Get the last-layer hidden states from the model output\n",
        "last_hidden_state = output.last_hidden_state\n",
        "\n",
        "# Compute document embeddings by mean pooling the word embeddings\n",
        "document_embeddings = torch.mean(last_hidden_state, dim=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 0.8316,  0.6865, -0.3569,  ...,  1.3948, -0.1248,  0.6666],\n",
              "        [ 0.8504,  0.6696, -0.4067,  ...,  1.4203, -0.1244,  0.6608],\n",
              "        [ 0.8850,  0.6766, -0.3495,  ...,  1.3402, -0.1016,  0.6599],\n",
              "        [ 0.8943,  0.6613, -0.3665,  ...,  1.3540, -0.1030,  0.6779]])"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "document_embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eCwxhv0GvOSl"
      },
      "outputs": [],
      "source": [
        "# Define the query document\n",
        "query_document = '我超級喜歡看球賽'\n",
        "\n",
        "# Encode the query document\n",
        "query_token_ids = tokenizer.encode(query_document, add_special_tokens=True, max_length=max_length, pad_to_max_length=True)\n",
        "query_tokens_tensor = torch.tensor([query_token_ids])\n",
        "\n",
        "# Get the BERT model output for the query document\n",
        "with torch.no_grad():\n",
        "    query_output = model(query_tokens_tensor)\n",
        "\n",
        "# Get the last-layer hidden states from the model output for the query document\n",
        "query_last_hidden_state = query_output.last_hidden_state\n",
        "\n",
        "# Compute the query document embedding by mean pooling the word embeddings\n",
        "query_document_embedding = torch.mean(query_last_hidden_state, dim=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[ 1.0193e+00,  6.8079e-01, -4.2355e-01,  1.2025e-01,  1.2165e+00,\n",
              "          2.1201e-01,  5.4072e-02,  1.8587e-01, -1.1844e-01, -1.4872e-01,\n",
              "          4.9715e-03,  1.8081e-01, -1.9555e-01, -8.7024e-01, -5.7365e-01,\n",
              "          8.9012e-01,  4.9895e-01, -5.3013e-01,  4.5205e-02, -4.8825e-01,\n",
              "         -2.1294e-01, -1.5432e+00, -3.3531e-01,  1.2212e-01,  6.5850e-01,\n",
              "         -3.7862e-01,  6.7309e-01, -9.1148e-01, -2.0345e+00,  4.8382e-01,\n",
              "          3.4822e-01,  1.2345e+00, -8.7425e-01, -1.2021e+00, -8.6886e-01,\n",
              "          2.3102e-01, -1.8267e-01, -3.5394e-01, -8.0446e-02,  2.3358e-01,\n",
              "          8.8538e-01, -6.8714e-01, -1.3028e-01, -1.4613e+00,  1.2544e-01,\n",
              "          8.3212e-01,  7.5333e-01,  6.1635e-01,  6.3930e-01,  9.9930e-01,\n",
              "         -7.4460e-01,  7.9773e+00, -3.5110e-01,  2.1924e-01, -7.3828e-01,\n",
              "         -1.3501e-01,  1.3776e+00, -3.1929e-01,  6.1334e-01, -4.9405e-02,\n",
              "          3.2238e-02,  4.2151e-01, -2.5599e-01, -3.6155e-01,  1.8321e-02,\n",
              "          8.3527e-01,  2.3358e-01, -6.7865e-01,  1.7837e+00,  4.3145e-01,\n",
              "         -5.3890e-01,  1.1928e+00,  5.2089e-01,  1.4068e-01,  3.9027e-01,\n",
              "         -8.0813e-01,  3.3446e-01, -6.1722e-01,  6.4920e-01,  7.4666e-02,\n",
              "         -7.8741e-01, -1.5415e-01, -7.9791e-01,  7.8415e-01, -1.3230e-01,\n",
              "          4.5668e-01, -8.9089e-02, -3.0198e-01, -5.1295e-01, -1.5257e+00,\n",
              "          1.3838e-01, -4.0444e-01, -9.1552e-01,  1.5751e-01,  2.7357e-01,\n",
              "         -4.8481e-01, -1.3006e-02,  6.1901e-01,  3.1475e-01,  9.2381e-01,\n",
              "         -7.4638e-01, -5.6784e-01,  5.9977e-01, -2.9660e-01, -2.5127e-01,\n",
              "          7.0880e-01,  4.2713e-01,  7.8899e-01,  1.5911e-01,  9.0625e-01,\n",
              "         -6.5202e-01,  7.7323e-02,  9.4249e-02, -6.2130e-01, -8.3824e-02,\n",
              "         -2.6174e-01,  2.3504e-02, -4.4164e-01,  1.0259e+00,  5.7963e-01,\n",
              "          7.8830e-01,  7.7101e-01,  4.6727e-01,  8.0395e-01, -6.9377e-01,\n",
              "          5.5450e-01,  3.1124e-02,  6.0237e-01, -9.1290e-01,  3.2480e-01,\n",
              "          9.2311e-02, -5.9144e-01,  5.4838e-01, -3.6571e-02,  3.6215e-01,\n",
              "         -3.7282e-01,  3.0612e-01, -4.1847e-01, -9.5099e-01,  6.6204e-01,\n",
              "          3.1724e-01, -1.5519e+00, -5.1468e-01,  5.0875e-01,  8.4807e-02,\n",
              "         -8.6831e-01, -5.4748e-01,  8.7892e-02,  3.0814e-01, -5.8201e-01,\n",
              "          2.9131e-01, -2.4159e-01,  6.1415e-01, -6.4183e-01, -1.0581e+00,\n",
              "         -9.0898e-01, -5.4864e-01, -6.7496e-01,  1.9488e-01,  8.1704e-02,\n",
              "         -3.5605e-01, -1.8486e-01,  2.1646e-01,  6.3415e-01,  9.8489e-01,\n",
              "          1.4467e-01, -1.9628e+00, -6.5564e-01,  9.0505e-01, -2.1116e-01,\n",
              "         -1.0180e+00,  5.9347e-01, -5.2786e-01, -7.1388e-01, -1.1804e+00,\n",
              "          3.4746e-01, -6.7534e-02, -6.9952e-01,  5.8486e-01,  3.6075e-01,\n",
              "         -3.8349e-01,  5.0027e-01, -3.6994e-01, -7.5193e-01, -2.6953e-02,\n",
              "         -3.3655e-01,  9.9611e-02,  8.0124e-01, -6.8020e-01,  1.3374e+00,\n",
              "          7.3464e-02,  9.7232e-01, -5.8053e-02, -1.7828e-02, -4.6868e-01,\n",
              "         -5.5345e-01,  3.7895e-01,  1.2253e+00,  5.3765e-01,  1.3258e+00,\n",
              "          8.6243e-01, -5.6976e-01,  1.1749e-01,  7.2889e-01,  1.3459e-01,\n",
              "          1.1574e-01, -5.8827e-01, -5.9020e-01, -7.5260e-01, -5.7349e-01,\n",
              "         -6.1144e-01,  4.9611e-01,  6.2071e-01,  2.8302e-01,  2.2555e-01,\n",
              "         -1.7818e+00, -4.3083e-02, -1.8548e-02,  8.0836e-01,  6.5048e-01,\n",
              "          4.3510e-02,  4.1030e-01,  4.1623e-01, -2.2797e-01, -3.3366e-02,\n",
              "         -3.2977e-01, -4.7883e-01, -5.8814e-01,  2.0368e-01, -1.9395e-01,\n",
              "         -7.8686e-01,  5.5714e-01, -1.1403e+00,  1.5366e-01, -3.9326e-01,\n",
              "         -1.4439e-01, -3.9918e-01,  6.7632e-02, -8.1774e-01, -4.5169e-01,\n",
              "         -1.9587e-01, -2.8607e-01, -7.7581e-02,  2.9987e-01, -9.6622e-02,\n",
              "          1.2643e-01, -1.5368e-01,  7.6955e-01, -1.2199e+00, -1.8764e-01,\n",
              "         -2.8818e-01,  9.8606e-01,  5.0601e-01,  2.7993e-02,  4.9521e-01,\n",
              "         -2.4902e-01, -4.4963e-01,  4.1034e-01,  3.0002e-01, -1.5150e-01,\n",
              "          3.9979e-01,  3.0125e-01,  9.4853e-01, -3.9542e-01,  3.8712e-01,\n",
              "          1.1483e+00,  8.3560e-01, -4.4294e-01,  1.0610e+00, -1.7936e-01,\n",
              "         -1.0453e+00, -3.5970e-01,  5.5607e-01, -1.0892e+00,  1.5099e-01,\n",
              "         -8.7011e-02, -1.2437e-01, -6.8592e-02, -8.8523e-01,  9.1650e-01,\n",
              "         -1.5555e-02, -3.2109e-01, -5.2480e-01, -6.6073e-01, -8.3358e-01,\n",
              "         -5.8908e-01,  4.4213e-01, -8.0526e-01,  6.1634e-01,  1.0081e-01,\n",
              "          7.0379e-01, -1.0802e+00,  3.6500e-01, -2.9514e-01, -3.6095e-01,\n",
              "         -8.9658e-01,  2.0398e-01, -5.2843e-01, -9.7387e-01,  6.7502e-01,\n",
              "          4.6725e-01, -1.4281e+00, -2.4966e-01,  1.1606e+00,  5.6287e-02,\n",
              "         -3.0377e-01, -5.4992e-02, -7.1917e-02,  5.2993e-01,  5.3052e-01,\n",
              "         -4.2901e-01, -1.5868e+00, -3.1714e-02,  2.4104e-01, -1.1193e+00,\n",
              "         -6.1164e-01,  1.1006e+00, -6.4191e-01,  3.3459e-01, -4.0684e-02,\n",
              "         -3.2431e-01, -1.3929e+00, -2.8266e-01,  9.7680e-01,  9.9955e-01,\n",
              "          1.0946e+00, -4.4880e-01,  8.0518e-02,  5.6496e-01, -9.5368e-01,\n",
              "          1.5127e-01, -3.8674e-01,  5.0710e-01, -7.4967e-01, -2.8812e-01,\n",
              "         -6.8175e-01,  2.2474e-01, -6.8155e-01,  2.9757e-02, -1.3123e+00,\n",
              "         -6.7276e-01, -4.8729e-01,  4.0603e-01,  9.1735e-01,  2.7870e-01,\n",
              "          1.6541e-03, -1.7492e+00,  2.9285e-01,  9.6452e-02,  3.2980e-02,\n",
              "         -1.6595e-01,  5.3031e-01,  5.0763e-02,  1.9809e-01,  4.8189e-01,\n",
              "          4.3757e-01, -6.9159e-01, -7.5579e-01,  1.2298e+00,  4.1336e-03,\n",
              "          6.8518e-01,  4.6556e-01, -3.3835e-01,  3.6074e-01,  1.3126e-01,\n",
              "         -5.3546e-02, -4.2341e-01, -1.7073e-01,  2.0732e-01,  8.1386e-02,\n",
              "         -2.0991e-01, -9.5760e-01, -4.0858e-01,  4.8159e-01,  4.9500e-01,\n",
              "         -2.7961e-01,  5.4683e-02,  4.3384e-01, -7.3259e-01,  4.2538e-01,\n",
              "         -7.5760e-01,  6.3417e-03, -4.7342e-01, -7.6491e-02, -2.0828e-01,\n",
              "         -2.5392e-01, -4.4175e-01, -4.0802e-01,  2.6443e-01, -5.2236e-01,\n",
              "         -1.0568e+00, -1.5068e-01, -5.0642e-01, -1.9671e+00,  4.0497e-01,\n",
              "          5.2110e-01,  3.5839e-01, -6.3399e-01,  8.1061e-01,  4.9015e-01,\n",
              "          8.1561e-01,  7.6913e-02,  1.3787e-01, -5.0433e-01,  1.6131e-01,\n",
              "         -6.3790e-01, -2.5108e-01, -4.1482e-01,  7.1909e-01,  2.6347e-01,\n",
              "         -1.0503e+00,  8.9927e-01,  9.5026e-01, -1.1277e+00,  5.3658e-01,\n",
              "          5.3302e-01,  8.7923e-01, -1.1626e+00, -1.2890e+00, -9.5923e-02,\n",
              "          2.1392e-01,  1.5989e-02, -6.9163e-04,  8.4064e-01,  1.5201e+00,\n",
              "          5.3724e-01,  3.0223e-01,  7.4157e-01,  5.2890e-01, -3.7363e-01,\n",
              "         -7.4284e-02,  8.2623e-02, -6.0652e-01, -1.8264e+00, -4.1792e-01,\n",
              "          5.2642e-01, -9.8758e-02,  5.6489e-01, -2.6474e-01,  1.9664e-01,\n",
              "         -1.0289e+00, -3.7925e-01,  3.1104e-01, -5.5657e-01, -7.9344e-01,\n",
              "          5.7492e-01, -7.6222e-01, -3.6505e-01,  5.3663e-01,  3.2161e-01,\n",
              "         -2.3638e-01, -7.6198e-01, -8.7646e-02,  1.2891e+00, -3.9426e-01,\n",
              "         -3.6472e-01, -3.9108e-01, -4.2563e-01,  6.2736e-01,  7.7367e-01,\n",
              "          9.6024e-01,  1.1478e+00, -9.3364e-01,  1.7648e-01,  7.3831e-01,\n",
              "         -1.7977e+00, -1.1694e+00,  3.2175e-02,  7.6931e-01,  1.0131e+00,\n",
              "          9.4801e-01,  3.4868e-01, -2.1754e-01, -8.2710e-01, -3.1071e-01,\n",
              "         -2.7656e-01, -7.8861e-01,  1.2074e+00, -1.5090e+00, -9.6984e-02,\n",
              "         -6.4447e-01, -7.3342e-02, -1.6324e+00, -9.2277e-02, -1.2253e+00,\n",
              "          9.7269e-03,  1.2262e+00, -4.6262e-01, -1.4509e+00,  1.1112e+00,\n",
              "         -7.7048e-01, -5.9257e-01, -7.3594e-02,  2.6905e-01, -5.9242e-01,\n",
              "         -4.4874e-01, -3.3574e-01, -3.7697e-01, -4.2320e-01,  1.2504e+00,\n",
              "         -3.4737e-01, -1.2656e+00, -3.5894e-01, -1.6859e-01,  3.2976e-01,\n",
              "         -1.4054e-01, -1.2653e-01,  5.0984e-01, -2.5719e-01,  1.5859e-01,\n",
              "          2.0275e-01, -1.0449e+00, -8.5989e-01,  7.4785e-01,  4.7191e-01,\n",
              "          8.5519e-01, -9.6654e-01,  1.0377e+00,  6.5046e-01, -1.7269e-02,\n",
              "          4.7137e-01,  1.9627e-01, -9.1347e-01,  2.5505e-01, -7.4087e-01,\n",
              "         -1.2653e-01,  7.0640e-01, -7.2963e-01,  1.6609e-01,  7.8077e-01,\n",
              "          2.0595e-01,  1.6872e-01, -2.5614e-01,  3.1642e-01,  1.9667e-01,\n",
              "          5.7981e-01, -8.5222e-01, -1.9110e-01, -2.5954e-02, -3.6017e-01,\n",
              "          9.6890e-02, -4.5981e-01,  2.0103e-01, -6.6829e-02,  6.4357e-02,\n",
              "         -1.5789e-01, -7.9257e-02,  1.8178e+00, -7.8035e-01,  6.8674e-01,\n",
              "         -6.3888e-01, -1.6495e+00,  3.8078e-01,  5.8207e-02,  1.6119e+00,\n",
              "          6.2658e-01, -1.1665e+00, -8.7113e-01, -1.4710e-02,  6.4910e-01,\n",
              "         -5.1227e-01,  1.2771e-02,  3.4606e-01,  5.8144e-01,  6.9071e-01,\n",
              "          5.0387e-01,  1.5058e-02, -6.1826e-01, -2.9842e-01,  5.1433e-01,\n",
              "          1.5516e-01,  2.4239e-01,  6.8681e-01, -6.1935e-01,  5.8683e-01,\n",
              "         -1.9128e-01,  1.4724e+00, -1.2222e+00,  4.8067e-01, -2.2036e-01,\n",
              "          4.4047e-01,  2.8815e-01,  9.5058e-01,  9.0120e-01, -5.1837e-01,\n",
              "         -6.0108e-01,  8.9207e-01,  5.0976e-02,  1.3732e-02, -5.9786e-01,\n",
              "         -8.7441e-01, -1.9096e-01,  1.0025e-01,  2.7341e-01, -1.5953e+00,\n",
              "         -5.1142e-01,  8.0794e-01, -2.4068e-01,  2.7623e-01, -5.1929e-01,\n",
              "         -1.0066e-01, -1.0322e+00, -6.0143e-02, -1.1796e-01,  7.4097e-01,\n",
              "         -5.0486e-02,  1.6296e-01, -3.2340e-01, -6.1043e-01,  1.1692e+00,\n",
              "          7.9072e-01, -2.8065e-01, -7.5956e-01, -1.4879e-01, -3.0172e-01,\n",
              "          9.0250e-01, -1.8013e-01,  1.6428e+00,  3.8054e-01, -1.0426e-01,\n",
              "         -1.0764e-02,  4.9393e-01, -4.6062e-01, -1.3011e-01,  6.5374e-01,\n",
              "          1.4142e+00,  8.8350e-01,  1.0737e+00, -8.9411e-01, -1.6405e+00,\n",
              "          3.4295e-01, -4.8304e-01,  9.0755e-01,  4.2415e-02, -2.5342e-01,\n",
              "          4.9428e-01,  4.3453e-01, -6.3178e-01, -4.1945e-01,  4.8821e-01,\n",
              "          1.3852e-01,  6.0579e-01, -1.8420e+00, -1.6398e-01,  8.9787e-02,\n",
              "         -2.0337e-01, -3.5746e-01, -4.0840e-01, -2.3418e-01, -8.0642e-01,\n",
              "          3.7037e-01, -3.9718e-01, -4.2431e-01, -7.9092e-01,  6.2060e-01,\n",
              "         -1.1030e+00,  1.3288e+00,  2.0574e+00,  2.9597e-01, -7.8101e-01,\n",
              "          8.6325e-01, -6.0867e-01, -2.9040e-01, -1.2114e+00,  2.4967e-01,\n",
              "          4.0322e-01,  1.8137e-01, -2.1176e-01,  1.9160e+00, -1.0375e+00,\n",
              "          5.4993e-02, -6.1839e-01,  3.6342e-01, -1.3654e+00, -2.1586e-02,\n",
              "         -3.6734e-01,  7.4384e-01, -1.4918e-01,  1.1948e-01,  6.7225e-02,\n",
              "         -3.1116e-01, -1.3215e+00, -6.1631e-01,  1.0272e+00, -1.5629e-01,\n",
              "         -2.1086e-01, -3.5638e-01,  5.5517e-01,  1.4992e+00,  5.7556e-01,\n",
              "          9.5671e-01, -1.6122e-01, -1.0565e-01, -3.9876e-01, -6.7688e-02,\n",
              "          8.8988e+00,  8.1914e-01,  1.1671e+00, -8.5107e-01,  5.1578e-01,\n",
              "         -1.1273e+00,  8.9720e-01,  4.0577e-01, -5.6982e-01, -5.4193e-01,\n",
              "         -5.7701e-02, -3.6242e-01,  1.4969e-01, -1.1355e+00,  6.6656e-03,\n",
              "          6.3813e-01, -1.6775e-01, -1.1197e+00, -7.4583e-01, -6.6577e-01,\n",
              "         -7.7458e-02, -1.2117e+00, -4.2832e-01,  6.3493e-02, -9.5291e-01,\n",
              "          9.0319e-01,  4.7600e-01, -7.1692e-01,  6.5583e-02,  6.1204e-01,\n",
              "         -9.4687e-02, -3.4499e-01,  6.2463e-01,  4.5911e-02, -2.9203e-01,\n",
              "          1.2081e+00, -4.7728e-01,  4.0167e-01,  6.1796e-01,  5.9812e-01,\n",
              "          7.0160e-01, -9.1307e-01,  1.2797e+00, -5.5012e-01,  1.4789e-01,\n",
              "          4.5780e-01, -1.5204e+00,  4.0183e-01, -9.0860e-01,  5.9710e-01,\n",
              "         -5.1763e-01,  7.0423e-01, -8.4389e-01, -8.3288e-01,  4.2075e-01,\n",
              "         -1.5381e+00,  6.5929e-01,  2.1974e-02,  2.7011e-02,  1.2185e-01,\n",
              "          4.0454e-01,  7.5845e-01,  4.9458e-01,  7.8466e-01, -3.0136e-01,\n",
              "         -2.1316e-01, -2.3217e-01, -1.1434e-01, -2.9802e-02, -8.3834e-01,\n",
              "          1.3311e+00, -2.8641e-02,  6.8409e-01]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "query_document_embedding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute the pairwise cosine similarity between the query document embedding and all document embeddings\n",
        "# [:,None]  --->  [1,2,3]->[[1],[2],[3]]\n",
        "# q.d / |q||d|\n",
        "query_cosine_similarities = np.dot(query_document_embedding, document_embeddings.T) / (np.linalg.norm(query_document_embedding, axis=1)[:, None] * np.linalg.norm(document_embeddings, axis=1))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find the top-k most similar documents to the query document\n",
        "# [::-1][:k]  -> [大到小][前k個]\n",
        "k = 5\n",
        "top_k_indices = np.argsort(query_cosine_similarities[0])[::-1][:k]\n",
        "print(top_k_indices)\n",
        "# Print the top-k most similar documents and their cosine similarity scores\n",
        "for i in top_k_indices:\n",
        "    print('Document: ', corpus[i])\n",
        "    print('Cosine similarity score: ', query_cosine_similarities[0][i])\n",
        "    print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
